{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b39a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\envs\\tf-embedding\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375401e",
   "metadata": {},
   "source": [
    "# Bertå‘é‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1301a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('F:\\Graduate materials\\è®ºæ–‡\\ä»£ç \\output_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa5cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['å•†å“ç»¼åˆä¿¡æ¯'] = df[['å•†å“åç§°', 'å•†å“åˆ†ç±»', 'å•†å“å“ç‰Œ']].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "df = df[~ (df['å•†å“ç»¼åˆä¿¡æ¯'].isna() | (df['å•†å“ç»¼åˆä¿¡æ¯'] == \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55adbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ‰€æœ‰å•†å“ä¿¡æ¯è¿›è¡Œæ¸…ç†\n",
    "# df[\"clean_text\"] = df[\"å•†å“ç»¼åˆä¿¡æ¯\"].apply(lambda x: re.sub('[^a-zA-Z0-9\\u4e00-\\u9fff]+', ' ', x))\n",
    "\n",
    "df[\"clean_text\"] = df[\"å•†å“ç»¼åˆä¿¡æ¯\"].apply(lambda x: re.sub('[^a-zA-Z\\u4e00-\\u9fff]+', ' ', x))\n",
    "\n",
    "# df[\"clean_text\"] = df['å•†å“ç»¼åˆä¿¡æ¯'].apply(lambda x:re.sub('[^\\u4E00-\\u9FD5]+',' ',x))\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feac5ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Acer\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        é˜¿è¿ªè¾¾æ–¯ ä¸‰å¶è‰ ç”· ç”°å¾„è¿åŠ¨ ä¼‘é—² ç«‹é¢† å¤¹å…‹ å¤–å¥— D TF STRP GN è¿åŠ¨ æˆ·å¤–...\n",
       "1        æ³¢ æ³¢é‹ é˜¿è¿ªè¾¾æ–¯ å®˜æ–¹ ç”·å¥³ æ¬¾ å¤å¤ ä¼‘é—² è€çˆ¹ é‹ OZWAVE JI è¿åŠ¨ æˆ·å¤– è¿...\n",
       "2        é˜¿è¿ªè¾¾æ–¯ ä¸‰å¶è‰ è¡— çƒé‹ ç”·å¥³ ç»å…¸ boost è¿åŠ¨é‹ STREETBALL II GX...\n",
       "3        ææƒ§ é²¨é±¼ é˜¿è¿ªè¾¾æ–¯ ä¸‰å¶è‰ ç”·å¥³ å¤å¤ è¿åŠ¨ è€çˆ¹ é‹ PROPHERE JI è¿åŠ¨ æˆ·å¤–...\n",
       "4        é˜¿è¿ªè¾¾æ–¯ ä¸‰å¶è‰ ç”·å¥³ æƒ…ä¾£ æ¬¾ å¤å¤é£ è¿åŠ¨ ç«‹é¢† å¤¹å…‹ å¤–å¥— adidas JI è¿åŠ¨ ...\n",
       "                               ...                        \n",
       "24353    WEIFANSHU é­å‡¡ èˆ’ æº å¿ƒ ç¥ç€ çœŸä¸ å¥—è£… å¤å­£ è½»å¥¢ æ°”è´¨ ä¸Šè¡£ åŠè£™ æ˜¾ç˜¦ ...\n",
       "24354    WEIFANSHU é­å‡¡ èˆ’ è½æ—¥ å›ä¿¡ åŠè£™ æœé¥° å†…è¡£ å¥³è£… åŠèº«è£™ WEIFANSHU...\n",
       "24355    WEIFANSHU é­å‡¡ èˆ’ è¢« çˆ± åŒ…è£¹ æ³°è¿ªç†Š å¤–å¥— éŸ©ç‰ˆ é•¿æ¬¾ å®½æ¾ é«˜æ¡£ å¥³è£… å†¬ ...\n",
       "24356    WEIFANSHU é­å‡¡ èˆ’ åç¯ éœ“è£³ ç‰›ä»” å¤–å¥— æœé¥° å†…è¡£ å¥³è£… çŸ­å¤–å¥— WEIFAN...\n",
       "24357    WEIFANSHU é­å‡¡ èˆ’ æ˜¥æ—¥ åº¦å‡ ç‰›ä»”è£¤ æœé¥° å†…è¡£ å¥³è£… è£¤å­ ç‰›ä»”è£¤ WEIFA...\n",
       "Name: fenci_with_jieba, Length: 24358, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # åŠ è½½ BERT Tokenizer\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# ğŸ”¹ å…ˆæå–è‹±æ–‡å•è¯ï¼Œé˜²æ­¢ jieba æ‹†åˆ†å®ƒä»¬\n",
    "def preserve_english(text):\n",
    "    # å…ˆæå–è‹±æ–‡å•è¯ï¼ˆå«æ•°å­—ï¼‰\n",
    "    english_words = re.findall(r'[a-zA-Z0-9]+', text)\n",
    "    # ç”¨ç‰¹æ®Šæ ‡è®°æ›¿æ¢è‹±æ–‡å•è¯ï¼ˆé˜²æ­¢ jieba æ‹†åˆ†ï¼‰\n",
    "    for i, word in enumerate(english_words):\n",
    "        text = text.replace(word, f\"ENG{i}\")  \n",
    "    \n",
    "    return text, english_words\n",
    "\n",
    "# ğŸ”¹ jieba åˆ†è¯ & è¿˜åŸè‹±æ–‡\n",
    "def jieba_process(text):\n",
    "    # æå–è‹±æ–‡å•è¯ï¼Œå¹¶æ›¿æ¢æˆå ä½ç¬¦\n",
    "    processed_text, english_words = preserve_english(text)\n",
    "\n",
    "    # ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "    words = [word for word in jieba.cut(processed_text) if word.strip()]\n",
    "\n",
    "    # è¿˜åŸè‹±æ–‡å•è¯\n",
    "    words = [english_words[int(w[3:])] if (w.startswith(\"ENG\") and w[3:].isdigit()) else w for w in words]\n",
    "\n",
    "#     # ç”¨ç©ºæ ¼æ‹¼æ¥ï¼Œä¿è¯ BERT Tokenizer è¯†åˆ«æ•´ä¸ªå•è¯\n",
    "#     tokenized = tokenizer.tokenize(\" \".join(words))\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# # ğŸ”¹ å¤„ç† DataFrame\n",
    "df['fenci_with_jieba'] = df[\"clean_text\"].apply(jieba_process)\n",
    "df['fenci_with_jieba']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a66723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24711 unique tokens.\n",
      "[[117, 348, 656, 10336, 28, 524, 575, 36, 170, 8538, 8539, 6402, 32, 34, 551, 28, 276, 32, 1831, 36, 179, 117], [3248, 4549, 117, 194, 181, 71, 84, 28, 443, 107, 4550, 1745, 32, 34, 88, 198, 135, 179, 117], [117, 348, 2937, 4241, 181, 104, 2708, 88, 4551, 1214, 1321, 32, 34, 88, 198, 135, 179, 117], [3750, 1384, 117, 348, 181, 84, 32, 443, 107, 3571, 1745, 32, 34, 88, 198, 135, 179, 117], [117, 348, 181, 315, 71, 3572, 32, 524, 575, 36, 179, 1745, 32, 34, 551, 28, 276, 32, 1831, 36, 179, 117]]\n",
      "Found 24711 unique tokens.\n",
      "padded_sequences å·²æˆåŠŸä¿å­˜ï¼\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"fenci_with_jieba\"].tolist()\n",
    "\n",
    "# å®ä¾‹åŒ–Tokenizerï¼Œè¿™é‡Œçš„num_wordså‚æ•°å¯ä»¥æ ¹æ®éœ€è¦è®¾ç½®ä¸ºè¯æ±‡è¡¨çš„æœ€å¤§å¤§å°\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(sequences[0:5])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# ğŸ”¹ 3. ç»Ÿä¸€åºåˆ—é•¿åº¦\n",
    "max_sequence_length = 8\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# ğŸ”¹ ä¿å­˜ä¸º .npy æ–‡ä»¶\n",
    "np.save(f\"..\\\\npy_document\\\\padded_sequences_{max_sequence_length}_length.npy\", padded_sequences)\n",
    "\n",
    "print(\"padded_sequences å·²æˆåŠŸä¿å­˜ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b1a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\envs\\transformers\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel\n",
    "from transformers import BertForPreTraining\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset as HFDataset\n",
    "import jieba\n",
    "import re\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec50336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_sequences å·²æˆåŠŸåŠ è½½ï¼\n",
      "Shape: (24358, 8)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ è¯»å– .npy æ–‡ä»¶\n",
    "padded_sequences = np.load(f\"..\\\\npy_document\\\\padded_sequences_8_length.npy\")\n",
    "\n",
    "print(\"padded_sequences å·²æˆåŠŸåŠ è½½ï¼\")\n",
    "print(\"Shape:\", padded_sequences.shape)  # æ£€æŸ¥æ•°æ®å½¢çŠ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6bc27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ..\\bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ åŠ è½½ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸åŠ è½½ BERT Tokenizerï¼‰\n",
    "bert_model_name = \"..\\\\bert-base-multilingual-cased\"\n",
    "bert_model = BertForPreTraining.from_pretrained(bert_model_name)\n",
    "\n",
    "# ğŸ”¹ è®¡ç®— BERT Embeddingsï¼ˆç”¨ BERT è®¡ç®— `padded_sequences` çš„ Embeddingsï¼‰\n",
    "input_ids = torch.tensor(padded_sequences)  # è½¬æ¢ä¸º PyTorch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67865fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids)\n",
    "\n",
    "# ğŸ”¹ è·å– `[CLS]` å¥å‘é‡ï¼ˆæˆ–è€…å–å¹³å‡æ± åŒ–ï¼‰\n",
    "sentence_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] å‘é‡\n",
    "print(\"BERT Embeddings Shape:\", sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf861b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ 7. ä¿å­˜å‘é‡åŒ–åçš„æ•°æ®\n",
    "np.save(f\".\\\\npy_document\\\\bert_multilingual_embeddings_{padded_sequences.shape[-1]}_length.npy\", sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7a2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ğŸ”¹ åŠ è½½ BERT é¢„è®­ç»ƒæ¨¡å‹\n",
    "# model_name = \"bert-base-chinese\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "# model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "\n",
    "# # ğŸ”¹ æå– BERT è¯å‘é‡\n",
    "# def get_bert_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state[:,0,:].squeeze().numpy()  # æå– [CLS] å‘é‡\n",
    "\n",
    "# # è®¡ç®—æ‰€æœ‰å•†å“çš„ BERT å‘é‡\n",
    "# embeddings = np.array([get_bert_embedding(text) for text in texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016edc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
