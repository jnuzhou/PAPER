{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b39a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\envs\\tf-embedding\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375401e",
   "metadata": {},
   "source": [
    "# Bert向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1301a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('F:\\Graduate materials\\论文\\代码\\output_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa5cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['商品综合信息'] = df[['商品名称', '商品分类', '商品品牌']].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "df = df[~ (df['商品综合信息'].isna() | (df['商品综合信息'] == \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55adbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有商品信息进行清理\n",
    "# df[\"clean_text\"] = df[\"商品综合信息\"].apply(lambda x: re.sub('[^a-zA-Z0-9\\u4e00-\\u9fff]+', ' ', x))\n",
    "\n",
    "df[\"clean_text\"] = df[\"商品综合信息\"].apply(lambda x: re.sub('[^a-zA-Z\\u4e00-\\u9fff]+', ' ', x))\n",
    "\n",
    "# df[\"clean_text\"] = df['商品综合信息'].apply(lambda x:re.sub('[^\\u4E00-\\u9FD5]+',' ',x))\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feac5ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Acer\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        阿迪达斯 三叶草 男 田径运动 休闲 立领 夹克 外套 D TF STRP GN 运动 户外...\n",
       "1        波 波鞋 阿迪达斯 官方 男女 款 复古 休闲 老爹 鞋 OZWAVE JI 运动 户外 运...\n",
       "2        阿迪达斯 三叶草 街 球鞋 男女 经典 boost 运动鞋 STREETBALL II GX...\n",
       "3        恐惧 鲨鱼 阿迪达斯 三叶草 男女 复古 运动 老爹 鞋 PROPHERE JI 运动 户外...\n",
       "4        阿迪达斯 三叶草 男女 情侣 款 复古风 运动 立领 夹克 外套 adidas JI 运动 ...\n",
       "                               ...                        \n",
       "24353    WEIFANSHU 魏凡 舒 溏 心 琥珀 真丝 套装 夏季 轻奢 气质 上衣 半裙 显瘦 ...\n",
       "24354    WEIFANSHU 魏凡 舒 落日 回信 半裙 服饰 内衣 女装 半身裙 WEIFANSHU...\n",
       "24355    WEIFANSHU 魏凡 舒 被 爱 包裹 泰迪熊 外套 韩版 长款 宽松 高档 女装 冬 ...\n",
       "24356    WEIFANSHU 魏凡 舒 华灯 霓裳 牛仔 外套 服饰 内衣 女装 短外套 WEIFAN...\n",
       "24357    WEIFANSHU 魏凡 舒 春日 度假 牛仔裤 服饰 内衣 女装 裤子 牛仔裤 WEIFA...\n",
       "Name: fenci_with_jieba, Length: 24358, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 加载 BERT Tokenizer\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# 🔹 先提取英文单词，防止 jieba 拆分它们\n",
    "def preserve_english(text):\n",
    "    # 先提取英文单词（含数字）\n",
    "    english_words = re.findall(r'[a-zA-Z0-9]+', text)\n",
    "    # 用特殊标记替换英文单词（防止 jieba 拆分）\n",
    "    for i, word in enumerate(english_words):\n",
    "        text = text.replace(word, f\"ENG{i}\")  \n",
    "    \n",
    "    return text, english_words\n",
    "\n",
    "# 🔹 jieba 分词 & 还原英文\n",
    "def jieba_process(text):\n",
    "    # 提取英文单词，并替换成占位符\n",
    "    processed_text, english_words = preserve_english(text)\n",
    "\n",
    "    # 用 jieba 进行中文分词\n",
    "    words = [word for word in jieba.cut(processed_text) if word.strip()]\n",
    "\n",
    "    # 还原英文单词\n",
    "    words = [english_words[int(w[3:])] if (w.startswith(\"ENG\") and w[3:].isdigit()) else w for w in words]\n",
    "\n",
    "#     # 用空格拼接，保证 BERT Tokenizer 识别整个单词\n",
    "#     tokenized = tokenizer.tokenize(\" \".join(words))\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# # 🔹 处理 DataFrame\n",
    "df['fenci_with_jieba'] = df[\"clean_text\"].apply(jieba_process)\n",
    "df['fenci_with_jieba']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a66723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24711 unique tokens.\n",
      "[[117, 348, 656, 10336, 28, 524, 575, 36, 170, 8538, 8539, 6402, 32, 34, 551, 28, 276, 32, 1831, 36, 179, 117], [3248, 4549, 117, 194, 181, 71, 84, 28, 443, 107, 4550, 1745, 32, 34, 88, 198, 135, 179, 117], [117, 348, 2937, 4241, 181, 104, 2708, 88, 4551, 1214, 1321, 32, 34, 88, 198, 135, 179, 117], [3750, 1384, 117, 348, 181, 84, 32, 443, 107, 3571, 1745, 32, 34, 88, 198, 135, 179, 117], [117, 348, 181, 315, 71, 3572, 32, 524, 575, 36, 179, 1745, 32, 34, 551, 28, 276, 32, 1831, 36, 179, 117]]\n",
      "Found 24711 unique tokens.\n",
      "padded_sequences 已成功保存！\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"fenci_with_jieba\"].tolist()\n",
    "\n",
    "# 实例化Tokenizer，这里的num_words参数可以根据需要设置为词汇表的最大大小\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# 将文本转换为序列\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(sequences[0:5])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# 🔹 3. 统一序列长度\n",
    "max_sequence_length = 8\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 🔹 保存为 .npy 文件\n",
    "np.save(f\"..\\\\npy_document\\\\padded_sequences_{max_sequence_length}_length.npy\", padded_sequences)\n",
    "\n",
    "print(\"padded_sequences 已成功保存！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b1a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\envs\\transformers\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel\n",
    "from transformers import BertForPreTraining\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset as HFDataset\n",
    "import jieba\n",
    "import re\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec50336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_sequences 已成功加载！\n",
      "Shape: (24358, 8)\n"
     ]
    }
   ],
   "source": [
    "# 🔹 读取 .npy 文件\n",
    "padded_sequences = np.load(f\"..\\\\npy_document\\\\padded_sequences_8_length.npy\")\n",
    "\n",
    "print(\"padded_sequences 已成功加载！\")\n",
    "print(\"Shape:\", padded_sequences.shape)  # 检查数据形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6bc27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ..\\bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 加载 BERT 预训练模型（不加载 BERT Tokenizer）\n",
    "bert_model_name = \"..\\\\bert-base-multilingual-cased\"\n",
    "bert_model = BertForPreTraining.from_pretrained(bert_model_name)\n",
    "\n",
    "# 🔹 计算 BERT Embeddings（用 BERT 计算 `padded_sequences` 的 Embeddings）\n",
    "input_ids = torch.tensor(padded_sequences)  # 转换为 PyTorch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67865fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids)\n",
    "\n",
    "# 🔹 获取 `[CLS]` 句向量（或者取平均池化）\n",
    "sentence_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] 向量\n",
    "print(\"BERT Embeddings Shape:\", sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf861b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 7. 保存向量化后的数据\n",
    "np.save(f\".\\\\npy_document\\\\bert_multilingual_embeddings_{padded_sequences.shape[-1]}_length.npy\", sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7a2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔹 加载 BERT 预训练模型\n",
    "# model_name = \"bert-base-chinese\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "# model.eval()  # 设为评估模式\n",
    "\n",
    "# # 🔹 提取 BERT 词向量\n",
    "# def get_bert_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state[:,0,:].squeeze().numpy()  # 提取 [CLS] 向量\n",
    "\n",
    "# # 计算所有商品的 BERT 向量\n",
    "# embeddings = np.array([get_bert_embedding(text) for text in texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016edc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
